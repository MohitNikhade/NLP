{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Stop Word Removal**"
      ],
      "metadata": {
        "id": "mL6v1xXejiph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common words that occur in sentences that add weight to the sentence are known as stop words. These stop words act as a bridge and ensure that sentences are grammatically correct.  \n",
        "In simple terms, words that are filtered out before processing natural language data is known as a stop word and it is a common pre-processing method."
      ],
      "metadata": {
        "id": "bdxOPfttjbKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required modules\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g314779nNjId",
        "outputId": "32d5d356-69e7-4144-b6b3-8fa4a8d39b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly we will import important libraries  \n",
        "Step 1: We will import required libraries and download stopwords we will need it further.  \n",
        "    \n",
        "import nltk  \n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "LoqKaa_Ht7rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing modles required for stopwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))  \n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoWCtxhUeDWX",
        "outputId": "ce06d094-0029-48bb-d3f7-adbe24aeccad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ourselves', \"wasn't\", 'before', 'both', 'yours', \"don't\", 'by', 'our', \"needn't\", \"hadn't\", \"that'll\", 'as', 'few', 'm', 'into', 'can', 'needn', 'does', 'the', 'over', 'its', 'he', 'own', \"you've\", 'above', \"wouldn't\", 'so', 'are', \"couldn't\", 'such', 'hers', 'has', 'themselves', 'is', \"isn't\", 'haven', 'didn', 'herself', \"should've\", 'then', 'on', 'while', \"doesn't\", 'itself', 'having', 'during', \"haven't\", 'you', \"mightn't\", 'his', \"it's\", 'wouldn', 'y', 'until', 'which', 'mustn', 'shouldn', 'because', 'about', 'nor', 'up', 'for', 'each', 't', 'at', 'in', 'him', 'they', 'wasn', \"you're\", 'under', 'theirs', 'against', 'himself', \"mustn't\", 'whom', \"you'll\", 'doing', 'other', 're', 'down', 'won', 'hadn', 'below', 'ours', 'where', 'how', 'yourselves', 'than', 'just', 'from', 'any', 'and', 'why', 'an', 'after', 'some', 'all', 'myself', 'been', 'off', 'we', 'were', 'when', 'those', 'am', 'i', \"didn't\", 'no', 'a', 'couldn', 'that', 'weren', 'your', 'their', 'yourself', 've', \"aren't\", 'through', 'or', 'these', 'be', 'if', 'but', 'of', 'd', 'ain', 'here', 'have', 'only', \"hasn't\", \"won't\", 'me', 's', 'now', 'will', \"you'd\", 'do', 'there', 'very', 'll', 'my', 'again', 'doesn', 'what', 'mightn', 'being', 'isn', \"shouldn't\", 'was', 'should', 'between', 'not', \"shan't\", 'shan', 'ma', 'she', 'it', \"she's\", 'them', 'this', 'o', 'aren', 'hasn', 'once', \"weren't\", 'did', 'further', 'same', 'out', 'with', 'had', 'her', 'to', 'most', 'more', 'too', 'who', 'don'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Now we will import 'stopwords' module. ** \n",
        "Then from module **nltk** we need to import module **'corpus'** and from corpus we need to import **'stopwords'**.  \n",
        "So directly we can do:  \n",
        "**from nltk.corpus import stopwords**  \n",
        "  \n",
        "    \n",
        "**Step 3: Setting up stopwords for language we are working with. ** \n",
        "  \n",
        "stop_words = set(stopwords.words('english'))   \n",
        "In this part we firstly created an object 'stop_words' in which we will be setting the stopwords words in english, it means it will get some stopwords after setting it.  \n",
        "As we are working with 'english' language we opted stopwords in english language as **'stopwords.words('english')'** and then we set it.  \n",
        "'set(stopwords.words('english'))'.  "
      ],
      "metadata": {
        "id": "L5_vDZU-vTZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Taking input from user for operating the stopwords removal from it\n",
        "\n",
        "data = input(\"Enter the sentence from which you want to remove stop words: \") \n",
        "demowords = [\"playing\", \"happiness\", \"going\", \"doing\", \"yes\", \"no\", \"I\", \"having\", \"had\", \"haved\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr9F_wfuem4c",
        "outputId": "50231f45-03ed-4350-ace5-35463834191c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the sentence from which you want to remove stop words: Machine Learning is one of the most trending field to work with. It needs data to give prediction by using the past scenarios\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Taking input from user:**  \n",
        "Taking input from user for performing stopwords operation on it.  \n",
        "We creared an object data for storing that input."
      ],
      "metadata": {
        "id": "i0WyEcZG1VM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing modules for tokenizing\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "#tokenizing data\n",
        "\n",
        "tokenize_words = word_tokenize(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umeBlSyEfYiE",
        "outputId": "cf0b5d97-f0c5-456b-b423-369c3b67047f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Tokenize the data:**  \n",
        "So for tokenization of data we need to import modules.  \n",
        "  \n",
        "**from nltk.tokenize import word_tokenize, sent_tokenize** \n",
        "  \n",
        "Then download **'punkt'** from nltk. \n",
        "  \n",
        "Then perform tokenization on data  \n",
        "**'tokenize_words = word_tokenize(data)'**  \n",
        "This will tokenize all the words present in our data. "
      ],
      "metadata": {
        "id": "7zHZz-5n8Yoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating empty list for our tokenized words without stopwords\n",
        "\n",
        "tokenize_words_without_stopwords = []"
      ],
      "metadata": {
        "id": "5BQuAMzzfyEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Creating a list for storing tokenized words with no stop words containing:**\n",
        "  \n",
        "**tokenize_words_wiithout_stopwords = []** "
      ],
      "metadata": {
        "id": "IL0Ho7KCB4iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing tokenized stopwords from our sentence/data\n",
        "\n",
        "for word in tokenize_words: \n",
        "  if word not in stop_words:\n",
        "    tokenize_words_without_stopwords.append(word)\n",
        "\n",
        "print(\"stop words which got removed: \", set(tokenize_words) - set(tokenize_words_without_stopwords))\n",
        "print(\"tokenize words which included all the words including stop words: \", tokenize_words) \n",
        "print(\"tokenize words without stop words: \", tokenize_words_without_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF5W50l2gn3q",
        "outputId": "616f8456-7b33-4947-89c2-9681af8a2774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stop words which got removed:  {'with', 'to', 'most', 'the', 'of', 'is', 'by'}\n",
            "tokenize words which included all the words including stop words:  ['Machine', 'Learning', 'is', 'one', 'of', 'the', 'most', 'trending', 'field', 'to', 'work', 'with', '.', 'It', 'needs', 'data', 'to', 'give', 'prediction', 'by', 'using', 'the', 'past', 'scenarios']\n",
            "tokenize words without stop words:  ['Machine', 'Learning', 'one', 'trending', 'field', 'work', '.', 'It', 'needs', 'data', 'give', 'prediction', 'using', 'past', 'scenarios']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7:  Obtaining tokenized stopwords, tokenized non stopwords, tokenized all words in data.**\n",
        "  \n",
        "As we are having a data with some sopwords. So we will remove those stopwords from data by using for loop. And in our data words which are not stop words will be stored in list we created before for no stop word containing words.  \n",
        "  \n",
        "So now we have tokenized words of data and tokenized words with no stopwords in data.  \n",
        "So by subtracting the tokenized words with no stopwords from tokenized words of data, we will get tokenized stopwords from data. "
      ],
      "metadata": {
        "id": "yoMvqR9bD-xy"
      }
    }
  ]
}
